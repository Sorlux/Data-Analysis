{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e51850-3038-461e-a96a-b31ba236ed1c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is primarly a resource for me to refer back to in regards to using Spark aswell as an introduction to spark in general. I will be explaining terms so that theoretically anyone should be able to understand specific details about Spark. This will be heavily based on this notebook: https://www.kaggle.com/code/kkhandekar/apache-spark-beginner-tutorial \n",
    "\n",
    "SparkSession is the main interface allowing the user to interact with Spark. The entry point to programming Spark with the Dataset and DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4ac633-30b4-45d2-af7d-cc7b83cc686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ldomi\\anaconda3\\envs\\xgboostENV\\python.exe\n",
      "C:\\Users\\ldomi\\anaconda3\\envs\\xgboostENV\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Classifier Libraries from Spark\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, NaiveBayes\n",
    "\n",
    "# Evaluation Libraries\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "\n",
    "# Features Libraries\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler, VectorIndexer, OneHotEncoder\n",
    "\n",
    "# Pipeline Library\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# DenseVector\n",
    "# Type of vector in MLib which stores all its values as a continous array (Basically a list or vector in C++)\n",
    "# For comparison, sparse vectors stores only non-zero values and their indices to save memory\n",
    "# A sparse vector holding [4.0,0.0,5.0,0.0,7.0] will represent it like so: Vectors.sparse(5, [0, 2, 4], [4.0, 5.0, 7.0]) \n",
    "# Indices 0, 2, 4 are non zero and there are 5 total elements\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import gc\n",
    "\n",
    "import os\n",
    "print(os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "print(os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5cb058-4d0a-4772-a093-d8f12db0b3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each config is a key value pair where the key is what is being accessed and the value is the modification amount\n",
    "spark = (SparkSession.builder\n",
    "        .appName('First Spark Project') # Set a name for the applcation which is shown in the web UI\n",
    "        .config(\"spark.executor.memory\", \"1G\") # 1 GB of memory is allocated for this executor\n",
    "        .config(\"spark.executor.cores\", \"4\") # # 4 CPU cores are allocated for this executor\n",
    "        .getOrCreate() # Either gets an existing SparkSesson or creates one if it doesn't exist\n",
    "       )\n",
    "spark.sparkContext.setLogLevel('INFO')\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d9ec73-ceab-49a8-9380-6108554e8103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: string, SepalLengthCm: string, SepalWidthCm: string, PetalLengthCm: string, PetalWidthCm: string, Species: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = './Datasets/iris.csv'\n",
    "\n",
    "# Reading the file in csv format and also confirming it has headers\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(file)\n",
    "\n",
    "# Faster reloading\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecd5b3a-61ff-40c7-a62e-419a76e70a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total records\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d053a657-f182-4975-9884-353251caa5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- SepalLengthCm: string (nullable = true)\n",
      " |-- SepalWidthCm: string (nullable = true)\n",
      " |-- PetalLengthCm: string (nullable = true)\n",
      " |-- PetalWidthCm: string (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data features and target\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8a0d09-a1fd-4bdc-b5ff-bd3f128d85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 records\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cea703f-99cb-4490-ad32-d69b7e295343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|        species|count|\n",
      "+---------------+-----+\n",
      "| Iris-virginica|   50|\n",
      "|    Iris-setosa|   50|\n",
      "|Iris-versicolor|   50|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing how many of each target we have\n",
    "data.groupBy('species').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70bcd24-5bbf-4335-93b9-00bda44dc9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|                Id|     SepalLengthCm|       SepalWidthCm|     PetalLengthCm|      PetalWidthCm|       Species|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|               150|               150|                150|               150|               150|           150|\n",
      "|   mean|              75.5| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|          NULL|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|          NULL|\n",
      "|    min|                 1|               4.3|                2.0|               1.0|               0.1|   Iris-setosa|\n",
      "|    max|                99|               7.9|                4.4|               6.9|               2.5|Iris-virginica|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d32baf-d0ce-4e81-b94b-5ca34c075878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|species_indx|\n",
      "+---+-------------+------------+-------------+------------+-----------+------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|         0.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|         0.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|         0.0|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|         0.0|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|         0.0|\n",
      "+---+-------------+------------+-------------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting the targets into numeric format\n",
    "# StringIndexer is a Spark MLib transformer used to convert categorical string values into numerical values\n",
    "# Setosa is 0.0, next is 1.0, next is 2.0\n",
    "# The values are actually assigned in descending order of frequency but in this case they all have the same frequency\n",
    "SIndexer = StringIndexer(inputCol='Species', outputCol='species_indx')\n",
    "data = SIndexer.fit(data).transform(data)\n",
    "\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a1abf9-9dad-4cbc-ab05-7597f39e2520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-------------+------------+\n",
      "|species_indx|SepalLengthCm|SepalWidthCm|PetalLengthCm|petalWidthCm|\n",
      "+------------+-------------+------------+-------------+------------+\n",
      "|         0.0|          5.1|         3.5|          1.4|         0.2|\n",
      "|         0.0|          4.9|         3.0|          1.4|         0.2|\n",
      "|         0.0|          4.7|         3.2|          1.3|         0.2|\n",
      "|         0.0|          4.6|         3.1|          1.5|         0.2|\n",
      "|         0.0|          5.0|         3.6|          1.4|         0.2|\n",
      "+------------+-------------+------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a separate dataframe to drop the Species Label and reorder\n",
    "df = data.select(\"species_indx\",\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"petalWidthCm\")\n",
    "\n",
    "#Inspect the dataframe\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924c4206-e525-417f-95d6-7d34bcf35e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are converting the datafram to an rdd using df.rdd\n",
    "# RDD stands for Resilient Distributed Dataset and it is an immutable distributed collection of elements\n",
    "# Basically RDDs are split into partitions allowing parallel ocmputation\n",
    "\n",
    "# .map applies the lambda function to each row of the  RDD\n",
    "# Spark MLlib expects data to be in the format (label, features) where the feature is a feature vector as a DenseVector type\n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "# Creating a new data fram to hold the inputs with headers\n",
    "df_indx = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "df_indx.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61742da1-69a4-4e82-a75a-a26346c800e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+\n",
      "|label|         features|     features_scaled|\n",
      "+-----+-----------------+--------------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|[6.15892840883878...|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|[5.9174018045706,...|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|[5.67587520030241...|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|[5.55511189816831...|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|[6.03816510670469...|\n",
      "+-----+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the standard scaler, specifying to scale the features DenseVector\n",
    "stdScaler = StandardScaler(inputCol = \"features\", outputCol = \"features_scaled\")\n",
    "\n",
    "# Fitting the standard scaler onto the dataframe\n",
    "scaler = stdScaler.fit(df_indx)\n",
    "\n",
    "# Transforming and outputting the scaled dataframe\n",
    "df_scaled = scaler.transform(df_indx)\n",
    "\n",
    "\n",
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d6f9e2-d3b7-4d24-a3a3-9ce8f3bff87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|     features_scaled|\n",
      "+-----+--------------------+\n",
      "|  0.0|[6.15892840883878...|\n",
      "|  0.0|[5.9174018045706,...|\n",
      "|  0.0|[5.67587520030241...|\n",
      "|  0.0|[5.55511189816831...|\n",
      "|  0.0|[6.03816510670469...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As we can see this adds a new columns therefore we must also drop it\n",
    "df_scaled = df_scaled.drop(\"features\")\n",
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9698416-5776-4eba-9cd3-e85fe3e784cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|     features_scaled|\n",
      "+-----+--------------------+\n",
      "|  0.0|[5.19282199176603...|\n",
      "|  0.0|[5.31358529390013...|\n",
      "|  0.0|[5.31358529390013...|\n",
      "|  0.0|[5.31358529390013...|\n",
      "|  0.0|[5.43434859603422...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_Set, testing_Set = df_scaled.randomSplit([0.9,0.1], seed = 42)\n",
    "training_Set.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb42dc6-6e24-4aad-a74f-2913a8fab302",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['Decision Tree','Random Forest','Naive Bayes']\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aa5e53d-0fca-43bc-86dc-4a3ede82a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol = \"label\", featuresCol= \"features_scaled\")\n",
    "dtc_model = dtc.fit(training_Set)\n",
    "\n",
    "dtc_pred = dtc_model.transform(testing_Set)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_acc = evaluator.evaluate(dtc_pred)\n",
    "model_results.extend([[model[0],'{:.2%}'.format(dtc_acc)]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56f40fd-c66f-49d4-8f56-7baf5910ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Random Forest Classifier --\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_scaled\", numTrees=10)          #instantiate the model\n",
    "rfc_model = rfc.fit(training_Set)                                                                     #train the model\n",
    "rfc_pred = rfc_model.transform(testing_Set)                                                           #model predictions\n",
    "\n",
    "#Evaluate the Model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_acc = evaluator.evaluate(rfc_pred)\n",
    "#print(\"Random Forest Classifier Accuracy =\", '{:.2%}'.format(rfc_acc))\n",
    "model_results.extend([[model[1],'{:.2%}'.format(rfc_acc)]])                                            #appending to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f59d425-f2b9-452b-9c27-746fa38cadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Naive Bayes Classifier --\n",
    "\n",
    "nbc = NaiveBayes(smoothing=1.0,modelType=\"multinomial\", labelCol=\"label\",featuresCol=\"features_scaled\")    #instantiate the model\n",
    "nbc_model = nbc.fit(training_Set)                                                                          #train the model\n",
    "nbc_pred = nbc_model.transform(testing_Set)                                                                #model predictions\n",
    "\n",
    "#Evaluate the Model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nbc_acc = evaluator.evaluate(nbc_pred)\n",
    "#print(\"Naive Bayes Accuracy =\", '{:.2%}'.format(nbc_acc))\n",
    "model_results.extend([[model[2],'{:.2%}'.format(nbc_acc)]])                                            #appending to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a85dbaea-5998-4e93-9a2c-d43b49d0779d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2328a394-d5ba-4f35-9646-97b74b31e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Models    Accuracy\n",
      "-------------------  ----------\n",
      "Decision Tree        77.78%\n",
      "Random Forest        77.78%\n",
      "Naive Bayes          88.89%\n"
     ]
    }
   ],
   "source": [
    "print (tabulate(model_results, headers=[\"Classifier Models\", \"Accuracy\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
